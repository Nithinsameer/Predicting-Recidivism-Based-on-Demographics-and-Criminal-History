---
title: "STAT FINAL PROJECT GROUP6"
author: "Rakesh Annamaneni"
date: "2023-05-10"
output: html_document
runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Research Question1

##1) Which factors are most significant in predicting recidivism three years after being released from prison?

```{r}
#importing required packages
library(randomForest)
library(tidyverse)
library(readr)
library(glmnet)
library(caret)
library(ROCR)
library(pROC)

```

```{r}
#importing the dataset
recidivism_df=read.csv("C:/Users/15714/Downloads/NIJ_s_Recidivism_Challenge_Full_Dataset (5).csv")
recidivism_df <- na.omit(recidivism_df)
str(recidivism_df)

```

```{r}
# Removing the unwanted and null variables and factoring them
recidivism_df<-subset(recidivism_df,select=-ID)
recidivism_df<-subset(recidivism_df,select=-Gender)
recidivism_df$Supervision_Risk_Score_First<-as.factor(recidivism_df$Supervision_Risk_Score_First)
recidivism_df <- recidivism_df %>% mutate_if(is.character, as.factor)
recidivism_df <- recidivism_df %>% mutate_if(is.logical, as.factor)
#Displaying the summary statistics and structure of the dataset
str(recidivism_df)
summary(recidivism_df)
```

#Displaying countplots of few Variables.

```{# {r}
# ui <- fluidPage(
#   selectInput("plot_select", label = "Select a plot", choices = c("Race",
#                                                                    "Gang Affiliated",
#                                                                    "Age at Release",
#                                                                    "Prior Arrest Episodes Felony",
#                                                                    "Prior Conviction Episodes Misd",
#                                                                    "Delinquency Reports",
#                                                                    "Supervision Risk Score First")),
#   
#   plotOutput("selected_plot")
# )
# 
# server <- function(input, output) {
#   
#   output$selected_plot <- renderPlot({
#     if (input$plot_select == "Gang Affiliated") {
#       plot_choice <- ggplot(recidivism_df, aes(x = Gang_Affiliated, fill = Recidivism_Within_3years)) + geom_bar(position = "dodge")
#     } else if (input$plot_select == "Age at Release") {
#       plot_choice <- ggplot(recidivism_df, aes(x = Age_at_Release, fill = Recidivism_Within_3years)) + geom_bar(position = "dodge")
#     } else if (input$plot_select == "Prior Arrest Episodes Felony") {
#       plot_choice <- ggplot(recidivism_df, aes(x = Prior_Arrest_Episodes_Felony, fill = Recidivism_Within_3years)) + geom_bar(position = "dodge")
#     } else if (input$plot_select == "Prior Conviction Episodes Misd") {
#       plot_choice <- ggplot(recidivism_df, aes(x = Prior_Conviction_Episodes_Misd, fill = Recidivism_Within_3years)) + geom_bar(position = "dodge")
#     } else if (input$plot_select == "Delinquency Reports") {
#       plot_choice <- ggplot(recidivism_df, aes(x = Delinquency_Reports, fill = Recidivism_Within_3years)) + geom_bar(position = "dodge")
#     } else if (input$plot_select == "Supervision Risk Score First") {
#       plot_choice <- ggplot(recidivism_df, aes(x = as.factor(Supervision_Risk_Score_First), fill = Recidivism_Within_3years)) + geom_bar(position = "dodge")
#     } else if (input$plot_select == "Race") {
#       plot_choice <- ggplot(recidivism_df, aes(x = Race, fill = Recidivism_Within_3years)) + geom_bar(position = "dodge")
#     }
#     
#     plot_choice
#   })
# }
# 
# shinyApp(ui = ui, server = server)

```

#Applying Various selection techniques and choosing the best model to list significant variables for predicting Recidivism

#Using Random Forest method to know the important variables and using top 10 varibles for building logistic Regression Model

```{r}
set.seed(1729)
trainIndex <- createDataPartition(recidivism_df$Recidivism_Within_3years, p = 0.8, list = FALSE)
training_set <- recidivism_df[trainIndex, ]
testing_set <- recidivism_df[-trainIndex, ]
set.seed(1729)
rf_model <- randomForest(Recidivism_Within_3years ~ ., data = training_set)
varImpPlot(rf_model)
important_vars <- row.names(varImp(rf_model)) %>% head(10)
log_reg_model_rf_selected <- glm(Recidivism_Within_3years ~ ., data = training_set[, c("Recidivism_Within_3years", important_vars)], family = binomial(link = "logit"))
glm_predicted_rf_selected <- predict(log_reg_model_rf_selected, testing_set[, important_vars], type = "response")
glm_predicted_rf_selected_b <- rep("FALSE", nrow(testing_set))
glm_predicted_rf_selected_b[glm_predicted_rf_selected >= .5] <- "TRUE"
confusionMatrix(as.factor(glm_predicted_rf_selected_b), testing_set$Recidivism_Within_3years)
# Building ROC Curve
library(ROCR)
pred_rf <- prediction(glm_predicted_rf_selected, testing_set$Recidivism_Within_3years)
roc_rf <- performance(pred_rf, measure = "tpr", x.measure = "fpr")
roc_obj <- roc(testing_set$Recidivism_Within_3years, glm_predicted_rf_selected)
auc_rf <- auc(roc_obj)
roc_df <- data.frame(fpr = roc_rf@x.values[[1]], tpr = roc_rf@y.values[[1]])
g1<-ggplot(roc_df, aes(x = fpr, y = tpr)) +
  geom_line(color = "green", size = 2) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", size = 1) +
  labs(title = "ROC Curve for Logistic Regression with Random Forest", x = "False Positive Rate", y = "True Positive Rate") +
  theme_classic() +
  theme(plot.title = element_text(face = "bold", size = 16),
        axis.title = element_text(face = "bold", size = 14),
        axis.text = element_text(size = 12),
        legend.position = "bottom",
        legend.title = element_blank(),
        legend.text = element_text(face = "bold", size = 12),
        panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(),
        panel.background = element_blank())+
  annotate("text", x = 0.7, y = 0.3, label = paste0("AUC = ", round(auc_rf, 3)), size = 5, color = "red")
```

##Using Lasso Regression for varibale selection and buiding a Logistic Regression model

```{r}
x_train <- model.matrix(Recidivism_Within_3years ~ ., data = training_set)[,-1]
y_train <- as.numeric(training_set$Recidivism_Within_3years) - 1
x_test <- model.matrix(Recidivism_Within_3years ~ ., data = testing_set)[,-1]
y_test <- as.numeric(testing_set$Recidivism_Within_3years) - 1
lasso_model <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 1)
plot(lasso_model)
coef <- coef(lasso_model, s = "lambda.min")
important_vars <- rownames(coef)[-1][coef[-1, 1] != 0]
log_reg_model_lasso_selected <- glm(Recidivism_Within_3years~Race+ Age_at_Release+ Gang_Affiliated +Supervision_Risk_Score_First+Supervision_Level_First+Education_Level+Dependents+Prison_Offense+Prison_Years+Prior_Arrest_Episodes_Felony+Prior_Arrest_Episodes_Misd+Prior_Arrest_Episodes_Violent+Prior_Arrest_Episodes_Property+Prior_Arrest_Episodes_Drug+Prior_Arrest_Episodes_PPViolationCharges+Prior_Conviction_Episodes_Felony+Prior_Conviction_Episodes_Misd+Prior_Conviction_Episodes_Prop+Delinquency_Reports+Program_Attendances+Program_UnexcusedAbsences+Residence_Changes+Avg_Days_per_DrugTest+DrugTests_THC_Positive+DrugTests_Cocaine_Positive+DrugTests_Meth_Positive+DrugTests_Other_Positive+Percent_Days_Employed+Jobs_Per_Year+Employment_Exempt, data = training_set, family = binomial(link = "logit"))
summary(log_reg_model_lasso_selected)
glm_predicted_lasso_selected <- predict(log_reg_model_lasso_selected, testing_set, type = "response")
glm_predicted_lasso_selected_b <- rep("FALSE", nrow(testing_set))
glm_predicted_lasso_selected_b[glm_predicted_lasso_selected >= .5] <- "TRUE"
confusionMatrix(as.factor(glm_predicted_lasso_selected_b), testing_set$Recidivism_Within_3years)
library(ROCR)
pred <- prediction(glm_predicted_lasso_selected, testing_set$Recidivism_Within_3years)
roc_lasso<- performance(pred, measure = "tpr", x.measure = "fpr")
roc_obj_l <- roc(testing_set$Recidivism_Within_3years, glm_predicted_lasso_selected)
auc_lasso <- auc(roc_obj_l)
g2<-ggplot(as.data.frame(roc_lasso@y.values[[1]]), aes(x = roc_lasso@x.values[[1]], y = roc_lasso@y.values[[1]])) +
  geom_line(color = "red", size = 1) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", size = 1) +
  ggtitle("ROC Curve for Logistic Regression with Lasso") +
  labs(x = "False Positive Rate", y = "True Positive Rate") +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.1)) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.1)) +
  theme_classic() +
  theme(plot.title = element_text(face = "bold", size = 16),
        axis.title = element_text(face = "bold", size = 14),
        axis.text = element_text(size = 12),
        legend.title = element_blank(),
        legend.text = element_text(face = "bold", size = 12)) +
  annotate("text", x = 0.7, y = 0.3, label = paste0("AUC = ", round(auc_lasso, 3)), size = 5, color = "red")
```

##Using Stepwise selection for varibale selection and buiding a Logistic Regression model

```{r include=FALSE}
library(MASS)
#full model 
full_model <- glm(Recidivism_Within_3years ~ ., data = training_set, family = binomial(link = "logit"))
#null model 
null_model <- glm(Recidivism_Within_3years ~ 1, data = training_set, family = binomial(link = "logit"))
#Perform stepwise variable selection
stepwise_model <- stepAIC(null_model, scope = list(lower = null_model, upper = full_model), direction = "both", trace = TRUE)
```

```{r}
glm.predicted_stepwise <- predict(stepwise_model, testing_set, type="response")
glm.predicted_s = rep("FALSE", nrow(testing_set))
glm.predicted_s[glm.predicted_stepwise >= 0.5] = "TRUE"
confusionMatrix(as.factor(glm.predicted_s), testing_set$Recidivism_Within_3years)
library(ROCR)
glm_prediction <- prediction(glm.predicted_stepwise, testing_set$Recidivism_Within_3years)
roc_step <- performance(glm_prediction, measure = "tpr", x.measure = "fpr")
roc_obj_s<- roc(testing_set$Recidivism_Within_3years, glm.predicted_stepwise)
auc_stepwise<- auc(roc_obj_s)
#Building ROC curve
roc_data_s <- data.frame(fpr = roc_step@x.values[[1]], tpr = roc_step@y.values[[1]])
roc_data_s$model <- "Stepwise Logistic Regression"
g3<-ggplot(roc_data_s, aes(x = fpr, y = tpr, color = model)) +
  geom_line(size = 1) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", size = 1) +
  labs(title = "ROC Curve for Stepwise Selection", x = "False Positive Rate", y = "True Positive Rate") +
  scale_color_manual(values = c("Stepwise Logistic Regression" = "blue")) +
  theme_classic() +
  theme(plot.title = element_text(face = "bold", size = 16),
        axis.title = element_text(face = "bold", size = 14),
        axis.text = element_text(size = 12))+
  annotate("text", x = 0.7, y = 0.3, label = paste0("AUC = ", round(auc_stepwise, 3)), size = 5, color = "blue")
  
```

```{r}
library(ROCR)

# Create prediction objects for each model
pred_rf <- prediction(glm_predicted_rf_selected, testing_set$Recidivism_Within_3years)
pred_lasso <- prediction(glm_predicted_lasso_selected, testing_set$Recidivism_Within_3years)
glm_prediction <- prediction(glm.predicted_stepwise, testing_set$Recidivism_Within_3years)

# Compute ROC curves for each model
roc_rf <- performance(pred_rf, measure = "tpr", x.measure = "fpr")
roc_lasso <- performance(pred_lasso, measure = "tpr", x.measure = "fpr")
roc_stepwise <- performance(glm_prediction, measure = "tpr", x.measure = "fpr")

roc_obj_rf<- roc(testing_set$Recidivism_Within_3years,glm_predicted_rf_selected )
roc_obj_lasso<- roc(testing_set$Recidivism_Within_3years, glm_predicted_lasso_selected)
roc_obj_stepwise<- roc(testing_set$Recidivism_Within_3years, glm.predicted_stepwise)
# Compute AUC for each model
auc_rf <- auc(roc_obj_rf)
auc_lasso <- auc(roc_obj_lasso)
auc_stepwise <- auc(roc_obj_stepwise)

roc_data <- data.frame(
  fpr = c(roc_rf@x.values[[1]], roc_lasso@x.values[[1]], roc_stepwise@x.values[[1]]),
  tpr = c(roc_rf@y.values[[1]], roc_lasso@y.values[[1]], roc_stepwise@y.values[[1]]),
  model = factor(rep(c("Random Forest", "Lasso", "Stepwise"), c(length(roc_rf@x.values[[1]]), length(roc_lasso@x.values[[1]]), length(roc_stepwise@x.values[[1]]))))
)

g4<-ggplot(roc_data, aes(x = fpr, y = tpr, color = model)) +
  geom_line(size = 1) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", size = 1) +
  labs(title = "ROC Curves for Logistic Regression Models", x = "False Positive Rate", y = "True Positive Rate") +
  scale_color_manual(values = c("Random Forest" = "green", "Lasso" = "red", "Stepwise" = "blue")) +
  theme_classic() +
  theme(plot.title = element_text(face = "bold", size = 16),
        axis.title = element_text(face = "bold", size = 14),
        axis.text = element_text(size = 12),
        legend.title = element_blank(),
        legend.text = element_text(face = "bold", size = 12))
```

## Choosing the best model from above models based on accuracy and ROC curve

```{# library(shiny)}
# library(ggplot2)
# library(ROCR)
# 
# # Load the data and create the logistic regression models here
# 
# # Create a list of the plots and their names
# plots <- list(
#   "ROC Curve for Logistic Regression with Lasso" =g2 ,
#   
#   "ROC Curve for Logistic Regression with Random Forest" = g1,
#   
#   "ROC Curve for Stepwise Selection" = g3,
#   
#   "ROC Curves for All Logistic Regression Models" =g4
# )
# 
# # Define the UI for the Shiny app
# ui <- fluidPage(
#   selectInput("plot", "Choose a plot:", choices = names(plots)),
#   plotOutput("plot_output")
# )
# 
# # Define the server for the Shiny app
# server <- function(input, output) {
#   output$plot_output <- renderPlot({
#     print(plots[[input$plot]])
#   })
# }
# 
# # Run the Shiny app
# shinyApp(ui = ui, server = server)
#  
```

From the above three models based on accuracy and AUC (Area Under ROC Curve) logistic regression with stepwise and Lasso Regression have almost same accuracy and AUC. When the accuracy and area under the ROC curve for both logistic regression by Lasso regression and logistic regression by stepwise selection are equal, we favor the model that is easier to understand. Lasso regression does this by minimizing the number of model variables while preserving acceptable performance, whereas stepwise selection might result in overfitting, yield unstable models, and rely on statistical significance tests. Lasso regression is the preferable option in this situation.

```{r}
summary(log_reg_model_lasso_selected)
```

##Conclusion

The most important factors in predicting recidivism three years after being released from jail are, according to the p-values in the summary:

Age at release: The likelihood that a person will commit another crime decreases with age. Age groups 28 to 32, 33 to 37, 38 to 42, and 43 to 47 all exhibit extremely low p-values, pointing to a significant correlation with recidivism.

Gang affiliation: People who are members of gangs are more likely to commit crimes again. This variable has a very low p-value, which suggests a high correlation with recidivism.

Supervision level: People who are under strict or specialized supervision are less likely to commit crimes again than those who are under regular supervision. These two factors have low p-values, which suggests a high correlation with recidivism.

Prior arrests: Having more arrests in the past, especially for violent or drug-related offenses, is linked to a higher risk of reoffending.

Delinquency records: People who have more delinquent records are more prone to commit crimes again. This variable's extremely low p-value suggests a high correlation with recidivism.

Drug test results: Reoffending is more likely when a drug test is positive, especially for meth.

Employment: People who work a larger proportion of the time and who work multiple jobs annually are less likely to commit crimes again. These two factors' extremely low p-values suggest a substantial correlation between them and recidivism.

#Reseach Question 2

##Are there racial differences in the supervision risk scores and levels upon release?

```{r}
risk_score_table <- table(recidivism_df$Race, recidivism_df$Supervision_Risk_Score_First)
print(risk_score_table)
```

```{r}
# Chi-square test for independence between Race and Supervision_Risk_Score_First
risk_score_chi_square <- chisq.test(risk_score_table)
print(risk_score_chi_square)
```

```{r}
# Cross-tabulation for Race and Supervision_Level_First
supervision_level_table <- table(data$Race, data$Supervision_Level_First)
print(supervision_level_table)
```

```{r}
# Chi-square test for independence between Race and Supervision_Level_First
supervision_level_chi_square <- chisq.test(supervision_level_table)
print(supervision_level_chi_square)
```

```{r}
library(shiny)
library(ggplot2)
library(rpart)

# Normalize the risk_score_table to get proportions
risk_score_props <- prop.table(risk_score_table, margin = 1)

# Convert the risk_score_props matrix into a data frame for plotting
risk_score_df <- as.data.frame(risk_score_props)
risk_score_df$Race <- rownames(risk_score_props)[risk_score_df$Var1]
colnames(risk_score_df) <- c("Race_Index", "Risk_Score", "Proportion", "Race")

# Normalize the supervision_level_table to get proportions
supervision_level_props <- prop.table(supervision_level_table, margin = 1)

# Convert the supervision_level_props matrix into a data frame for plotting
supervision_level_df <- as.data.frame(supervision_level_props)
supervision_level_df$Race <- rownames(supervision_level_props)[supervision_level_df$Var1]
colnames(supervision_level_df) <- c("Race_Index", "Supervision_Level", "Proportion", "Race")

# Define the UI
ui <- fluidPage(
  titlePanel("Supervision Data"),
  sidebarLayout(
    sidebarPanel(
      selectInput("plot_type", label = "Choose a plot to display",
                  choices = c("Supervision Risk Scores by Race", "Supervision Levels by Race"))
    ),
    mainPanel(
      plotOutput("plot")
    )
  )
)

# Define the server
server <- function(input, output) {
  output$plot <- renderPlot({
    if (input$plot_type == "Supervision Risk Scores by Race") {
      ggplot(risk_score_df, aes(x = Risk_Score, y = Proportion, fill = Race)) +
        geom_bar(stat = "identity", position = "dodge") +
        labs(title = "Supervision Risk Scores by Race",
             x = "Supervision Risk Score",
             y = "Proportion") +
        theme_minimal()
    } else if (input$plot_type == "Supervision Levels by Race") {
      ggplot(supervision_level_df, aes(x = Supervision_Level, y = Proportion, fill = Race)) +
        geom_bar(stat = "identity", position = "dodge") +
        labs(title = "Supervision Levels by Race",
             x = "Supervision Level",
             y = "Proportion") +
        theme_minimal()
    }
  })
}

# Run the app
shinyApp(ui = ui, server = server)

```

##Conclusion:

Based on the given outputs, there are significant racial differences in both the supervision risk scores and levels upon release:

Supervision Risk Score: The p-value for the Chi-square test between race and supervision risk score is less than 2.2e-16, which is much smaller than the significance level of 0.05. Therefore, you can reject the null hypothesis and conclude that there is a significant relationship between race and supervision risk score, indicating racial differences in the supervision risk scores upon release.

Supervision Level: The p-value for the Chi-square test between race and supervision level is 3.569e-06, which is also smaller than the significance level of 0.05. This means you can reject the null hypothesis and conclude that there is a significant relationship between race and supervision level, indicating racial differences in the supervision levels upon release.

In summary, the results show that there are significant racial differences in both the supervision risk scores and levels upon release.

Supervision Risk Score: For lower risk scores (1, 2, 3), the proportion of WHITE individuals is higher than the proportion of BLACK individuals. For mid-range risk scores (4, 5, 6, 7), the proportion of BLACK individuals is higher than the proportion of WHITE individuals. For higher risk scores (8, 9, 10), the proportions are more balanced between the two racial groups, with the BLACK individuals still having a higher proportion in scores 8 and 9.

Supervision Level: For the High supervision level, the proportion of BLACK individuals is higher than the proportion of WHITE individuals.

For the Specialized supervision level, the proportion of BLACK individuals is higher than the proportion of WHITE individuals. For the Standard supervision level, the proportion of BLACK individuals is higher than the proportion of WHITE individuals.

Based on these observations, there are differences in how race impacts supervision risk scores and levels upon release. The data shows that BLACK individuals have a higher proportion of higher risk scores and more restrictive supervision levels (High and Specialized). However, it's essential to remember that correlation does not imply causation, and these findings should be interpreted cautiously. It's important to consider other factors and contextual information when assessing racial disparities in supervision risk scores and levels upon release.

#Reseach Question3

##Does the possibility of recidivism anyway related with employment history,including the number of jobs held annually and the percentage of days worked?

```{r}
library(rpart)

fit <- rpart(Recidivism_Within_3years ~ Jobs_Per_Year + Percent_Days_Employed, data = recidivism_df, method = "class")
summary(fit)
```

```{r}
library(rpart.plot)

rpart.plot(fit, yesno=2, type=0, extra=1)
```

```{r}

correlationMatrix <- cor(recidivism_df[c("Recidivism_Within_3years", "Jobs_Per_Year", "Percent_Days_Employed")], use = "complete.obs")
print(correlationMatrix)

```

##Conclusion

The correlation matrix and the decision tree output provide a comprehensive understanding of the relationship between the probability of recidivism and employment history, including the number of jobs per year and the percentage of days employed.

From the correlation matrix, it can be inferred that both the number of jobs per year and the percentage of days employed are negatively correlated with recidivism. The correlation between recidivism and the number of jobs per year is -0.051, indicating a weak negative correlation. On the other hand, the correlation between recidivism and the percentage of days employed is stronger at -0.267, indicating a moderate negative correlation. This suggests that the likelihood of recidivism decreases as the number of jobs per year and the percentage of days employed increase. However, it's important to note that correlation does not imply causation.

The decision tree output provides a more nuanced view of the relationship between these variables. From the variable importance section, it appears that the number of jobs per year (51%) and the percentage of days employed (49%) are almost equally important in predicting recidivism. This means that both variables significantly contribute to the model's ability to accurately predict recidivism.

Looking at the decision tree's splits, it's clear that certain thresholds in the number of jobs per year and the percentage of days employed lead to different predictions about recidivism. For instance, the first split is based on whether the percentage of days employed is less than approximately 0.643. If it is, the decision tree predicts a lower likelihood of recidivism (node 2), whereas if it's greater, the decision tree predicts a higher likelihood of recidivism (node 3). This suggests that there's a non-linear relationship between these variables and recidivism, which wouldn't be captured by a simple correlation coefficient.

In conclusion, while there is a negative correlation between recidivism and both the number of jobs per year and the percentage of days employed, the relationship is complex and non-linear. As such, an increase in the number of jobs per year and the percentage of days employed doesn't necessarily lead to a decrease in the likelihood of recidivism in a linear fashion. The decision tree model provides a more detailed understanding of this relationship, demonstrating that certain thresholds in these variables can lead to different predictions about recidivism.
